챕터 18. RNN에 대한 내용입니다.

이 챕터는 RNN이 무엇인가에 대해 알아보고 / 뉴스 데이터와 영화 데이터를 / RNN을 채택한 모델을 구성해보도록 하겠습니다.

챕터 17에서 이용한 자연어 처리 모델은 / 특정 단어의 사용에 민감한 형태였습니다. / 다시 말해, 특정 단어에 사용이 / 출력에 영향을 미치는 구조였습니다.<br />
그러나 RNN은 단어뿐만 아니라 / 단어 간 관계도 학습에 이용할 수 있습니다.

이를 위해 앞선 단어의 연산값을 / 다음 반복에서 이용합니다.<br />
당연히 앞선 단어가 무엇이었냐에 따라 / 다른 값을 받습니다.

예를 들어 / 어제와 오늘의 주가를 묻는 질문에서 / 오늘이라는 단어의 처리값과 / 어제라는 단어의 처리값은 다른 뜻을 가진 단어이므로 / 다른 값을 다음으로 넘길 겁니다.

이전까지의 신경망을 왼쪽으로 표현한다면 / RNN은 단어의 수만큼 한 층을 돌아야 하므로 / 그 층을 반복하고 순환하는 형태로 볼 수 있습니다.<br />
그래서 이 모델을 순환 신경망, RNN이라고 부릅니다.

RNN은 다음과 같이 다양한 형태를 가질 수 있습니다.<br />
가장 왼쪽에 있는 것은 단순한 형태의 신경망을 표현한 것이고 / 그 다음은 출력을 여러 개로 했을 때로, / 이미지를 보고 그 이미지를 설명하는 텍스트를 만들어내는 모델을 예로 들 수 있습니다. / 세번쨰는 정확히 반대의 형태이고 / 마지막으로 출력도 입력도 많은 건 번역기를 예로 들 수 있습니다.

개념으로 돌아가서 / RNN은 한 계층을 문장의 단어 수만큼 반복하는 것으로 볼 수 있습니다.<br />
이는 단어 간 관계를 모델이 이해할 수 있는 동시에 / 계층이 많아지는 효과를 내면서 기울기 소실이 일어날 가능성을 높입니다.

그래서 RNN 개념을 이용할 때는 / LSTM 기법을 많이 이용합니다.<br />
LSTM은 다음 반복이 시작될 때 / 이전에 계산된 값을 이용할지 이용하지 않을지를 결정해 기울기 소실의 발생 가능성을 줄입니다.

(마우스 이용해야 한다, 정신 차려)<br />
앞선 그림을 통해 설명하자면 / 다음 반복을 시작할 때, A 내부에서 / 이전 값을 이용할지 이용하지 않을지 결정하고 / A 연산을 진행합니다.

---

이제 RNN 특히 LSTM을 이용한 모델을 구성해보겠습니다.<br />
이용하는 데이터는 로이터 뉴스 데이터로 / 11,228 개의 뉴스 기사와 각 뉴스 기사의 카테고리 데이터입니다. 

tensorflow 패키지를 통해 이용할 수 있는 데이터이고 / 직접 확인해보면 / 학습용 데이터와 테스트용 데이터가 나뉘어 있고, / 이미 토큰화 과정을 진행한 후임을 알 수 있습니다.<br />
더불어 load_data 함수의 num_words 매개변수는 / 해당 데이터의 단어 사용 빈도를 내림차순으로 정렬했을 때 1000번째 단어까지만 / 가져오게 합니다.

다시, 자연어를 input으로 하는 모델은 / 다음과 같은 단계의 전처리 과정을 / 거치게 됩니다.

이번에 이용하고자 하는 데이터는 / 이미 토큰화가 진행된 후임으로

패딩과 원-핫 인코딩만을 진행하면 됩니다.

이후 모델 구성에서 이전과의 차이점은 / Embedding 이후, LSTM 계층이 있다는 것과 / 해당 계층의 활성화 함수로 탄젠트 함수를 이용한다는 점입니다.

탄젠트 함수는 ReLU나 Sigmoid와 달리 음수값도 갖습니다.<br />
더불어 중심이 0으로 이동하면서 sigmoid의 최적화 과정이 느려지는 문제를 해결했습니다.

이 모델을 코드로 구현해보면

앞서처럼 데이터를 불러와서 / sequence에 pad_sequence 함수를 통해 길이를 100으로 맞추는 패딩과 / to_categorical 함수를 통한 원-핫 인코딩을 진행합니다.

이후엔 이전처럼 Sequential 클래스를 이용해 / Embedding, LSTM, Dense 계층을 구성합니다.<br />
loss도 이전처럼 cross entropy로, / 그리고 adam 기법을 통해 모델 accuracy를 향상시키도록 최적화합니다.<br />
이 모형은 매번 100개씩 데이터를 뽑아 가중치를 업데이트하는 방법을 / 총 20번 반복하는 방식으로 학습을 진행했습니다.<br />
해당 모형의 train accuracy는 71%이며

모형에서 학습의 loss를 받아와 / matplot 라이브러리를 통해 시각화해보면

validation set에 대한 loss는 16번째 epoch에서 / 최솟값을 가졌고, 이후에는 값이 개선되지 않았음을 알 수 있습니다.

---

다음으로 LSTM 뿐만 아니라 CNN도 이용하여 모델을 구성해보겠습니다.<br />
이용하는 데이터는 IMDB이라는 / 영화 리뷰 데이터이며, 텍스트 뿐만 아니라 / 긍정적인 평가인지 부정적인 평가인지를 나타내는 카테고리가 있습니다.<br />
해당 데이터 또한 tensorflow 패키지를 통해 불러올 수 있는 데이터이므로 / 이미 토큰화되어 있습니다.

그래서 빠르게 모델 구성으로 넘어가면 / 주된 변화는 Dropout, Convolutional, Pooling 계층이 추가된 것과 / 탄젠트와 softmax였던 activation 함수가 / ReLU, Sigmoid로 바뀐 것입니다.

그중에서도 Convolutional 계층과 Pooling 계층에 주목해보자면

기존에 이용한 Convolutional 계층은 / 입력 데이터가 이미지였기 때문에 / 색채에 대한 차원을 제외하면 / 2차원의 형태를 띠고 있었습니다.

그러나 이번 데이터는 text 데이터이기 때문에 / embedding으로 인한 차원을 제외하면 / 1차원 데이터가 됩니다.<br />
따라서, 이전에 이용한 Conv2D 함수가 아니라 Conv1D 함수를 이용해야 합니다.

이는 Pooling도 마찬가지입니다. / 입력 데이터의 차원이 하나 낮아졌으므로 / 그에 맞는 함수를 이용해야 합니다.

이 모델을 코드로 구현해보면

imdb 데이터를 불러와서 / sequence에 pad_sequence 함수를 통해 길이를 100으로 맞추기만 합니다.<br />
이전 데이터에선 label에 대해 원-핫 인코딩을 진행했지만, / 이번 데이터는 출력 값이 긍정 혹은 부정이므로 / 원-핫 인코딩을 이용하지 않아도 됩니다.

이후엔 이전처럼 Sequential 클래스를 이용해 / Embedding, Dropout, Convolutional, MaxPooling, LSTM, Dense 계층을 구성합니다.<br />

이렇게 구성된 모형은 summary 멤버함수를 통해 / 그 형태를 볼 수 있습니다.<br />
이때 형태란 output의 shape을 간략하게 표현한 것을 말합니다.

모델 구성을 계속 진행해보면 / 이 모형은 앞선 뉴스 데이터와 마찬가지로 / loss를 cross entropy로 계산하고, / adam 기법을 통해 모델 accuracy를 향상시키도록 최적화합니다.<br />
이 모형은 매번 100개씩 데이터를 이용하며 / 5번 전체 반복하는 방식으로 학습을 진행합니다.<br />
해당 모형의 train accuracy는 85%이며

모형에서 학습의 loss를 받아와 / matplot 라이브러리를 통해 시각화해보면

validation set에 대한 loss는 2번째 epoch 이후부턴 개선되지 않고 있으며 / epoch를 더 키웠을 때도 validation set에 대한 loss가 조금씩이라도 증가한다면 / overfitting을 의심할 수 있을 것 같습니다.

이것으로 RNN 챕터 발표를 마칩니다.
