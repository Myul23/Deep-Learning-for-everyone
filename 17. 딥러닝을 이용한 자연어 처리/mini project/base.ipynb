{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('checking_tf_2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e4d01ba02fecad236d245cb7520d8449a6deae080ff3a433ae4973db8259913a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## NLP mini-project\n",
    "\n",
    "- kaggle: https://www.kaggle.com/samdeeplearning/deepnlp?select=Sheet_1.csv\n",
    "- 이런 데이터도 있었다. https://www.kaggle.com/marklvl/sentiment-labelled-sentences-data-set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- 데이터 이해\n",
    "- 문장 전처리 (int factor로 변환, 길이 맞추기-padding)\n",
    "- word embedding을 포함한 모형 설계\n",
    "- training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 데이터 이해\n",
    "\n",
    "Sheet1.csv contains 80 user responses, in the responsetext column, to a therapy chatbot. Bot said: 'Describe a time when you have acted as a resource for someone else'.  User responded. If a response is 'not flagged', the user can continue talking to the bot. If it is 'flagged', the user is referred to help.\n",
    "* Sheet1.csv에는 치료 챗봇에 대한 80개의 사용자 응답이 응답 텍스트 열에 포함되어 있습니다. Bot은 다음과 같이 말했다: '당신이 다른 사람을 위한 자원 역할을 했던 때를 묘사하라.' 사용자가 응답했습니다. 응답에 플래그가 지정되지 않은 경우 사용자는 계속해서 봇과 대화할 수 있습니다. 플래그가 '플래그'된 경우 사용자에게 도움말을 참조합니다.\n",
    "\n",
    "Sheet2.csv contains 125 resumes, in the resumetext column. Resumes were queried from Indeed.com with keyword 'data scientist', location 'Vermont'. If a resume is 'not flagged', the applicant can submit a modified resume version at a later date. If it is 'flagged', the applicant is invited to interview.\n",
    "* 시트 2.csv에는 재개 텍스트 열에 125개의 이력서가 들어 있습니다. 이력서는 Indeed.com에서 키워드 '데이터 사이언티스트', 위치 'Vermont'로 조회되었다. 이력서에 플래그가 지정되지 않은 경우, 신청자는 나중에 수정된 이력서 버전을 제출할 수 있습니다. 플래그가 '플래그'된 경우 지원자는 인터뷰에 초대됩니다.\n",
    "\n",
    "이걸로 뭘 해야 하죠?\n",
    "새 재개/응답은 플래그가 지정되거나 플래그가 지정되지 않은 것으로 분류합니다.\n",
    "여기에는 이력서와 응답이라는 두 가지 데이터 세트가 있습니다. 데이터를 열차 세트와 테스트 세트로 분할하여 분류기의 정확도를 테스트합니다. 두 문제에 동일한 분류기를 사용하기 위한 보너스 포인트."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이용한 package 모음\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "df = pd.read_csv(\"C:/Github Projects/DL-for-All/17. 딥러닝을 이용한 자연어 처리/mini project/0. data/Sheet_1.csv\")\n",
    "# df = pd.read_csv(\"C:/Github Projects/DL-for-All/17. 딥러닝을 이용한 자연어 처리/mini project/0. data/Sheet_2.csv\", encoding='unicode_escape')\n",
    "print(df.info())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 891,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 80 entries, 0 to 79\nData columns (total 8 columns):\nresponse_id      80 non-null object\nclass            80 non-null object\nresponse_text    80 non-null object\nUnnamed: 3       2 non-null object\nUnnamed: 4       0 non-null float64\nUnnamed: 5       1 non-null object\nUnnamed: 6       0 non-null float64\nUnnamed: 7       1 non-null object\ndtypes: float64(2), object(6)\nmemory usage: 5.1+ KB\nNone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  response_id        class                                      response_text  \\\n",
       "0  response_1  not_flagged              I try and avoid this sort of conflict   \n",
       "1  response_2      flagged  Had a friend open up to me about his mental ad...   \n",
       "2  response_3      flagged  I saved a girl from suicide once. She was goin...   \n",
       "3  response_4  not_flagged  i cant think of one really...i think i may hav...   \n",
       "4  response_5  not_flagged  Only really one friend who doesn't fit into th...   \n",
       "\n",
       "  Unnamed: 3  Unnamed: 4 Unnamed: 5  Unnamed: 6 Unnamed: 7  \n",
       "0        NaN         NaN        NaN         NaN        NaN  \n",
       "1        NaN         NaN        NaN         NaN        NaN  \n",
       "2        NaN         NaN        NaN         NaN        NaN  \n",
       "3        NaN         NaN        NaN         NaN        NaN  \n",
       "4                    NaN        NaN         NaN        NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response_id</th>\n      <th>class</th>\n      <th>response_text</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n      <th>Unnamed: 5</th>\n      <th>Unnamed: 6</th>\n      <th>Unnamed: 7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>response_1</td>\n      <td>not_flagged</td>\n      <td>I try and avoid this sort of conflict</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>response_2</td>\n      <td>flagged</td>\n      <td>Had a friend open up to me about his mental ad...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>response_3</td>\n      <td>flagged</td>\n      <td>I saved a girl from suicide once. She was goin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>response_4</td>\n      <td>not_flagged</td>\n      <td>i cant think of one really...i think i may hav...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>response_5</td>\n      <td>not_flagged</td>\n      <td>Only really one friend who doesn't fit into th...</td>\n      <td></td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 892
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "source": [
    "#### tensorflow 이용을 위해 numpy package로 불러오기\n",
    "\n",
    "- label만 하면 되는 거였다.\n",
    "\n",
    "df = np.loadtxt(\"C:/Github Projects/DL-for-All/17. 딥러닝을 이용한 자연어 처리/mini project/0. data/Sheet_1.csv\", delimiter=\",\", dtype=np.str, usecols=(1, 2, 3, 5, 7), skiprows=1)<br />\n",
    "print(df.shape, df\\[0:5\\], sep=\"\\n\")\n",
    "\n",
    "sentenses = df\\[:, 1:\\]<br />\n",
    "labels = df\\[:, 0\\]<br />\n",
    "print(sentenses.shape, sentenses\\[0:5\\], labels.shape, labels\\[0:5\\], sep=\"\\n\\n\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 전처리"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "print(tf.__version__)"
   ]
  },
  {
   "source": [
    "1. split to sentenses and label, Ont-hot Encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "한 번에 다 바꿔버리는 게 무식한 방법일까 / batch 뽑을 때마다 뽑은 것만 바꾸는 게 무식한 방법일까\n",
    "\n",
    "처음은 한 번만 돌려도 되지만, 데이터가 많아지면 시간 엄청 걸릴 것이고,<br />\n",
    "다른 건 한 번에 조금만 돌리지만, 반복해서 돌려지는 게 있을 것이고, 무엇보다 epoch를 이용하면서 어차피 다 돌리게 됨.\n",
    "\n",
    "학습은 꽤 느린 방법론이니까 최대한 시간을 줄이는 방법으로 가는 게 맞을 것으로 보임.<br />\n",
    "따라서 1번 선택"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " I try and avoid this sort of conflict\n"
     ]
    }
   ],
   "source": [
    "sentenses = []\n",
    "for row in np.arange(df.shape[0]):\n",
    "    sentense = \"\"\n",
    "    bases = df.iloc[row, :].isna()\n",
    "    for col in np.arange(2, df.shape[1]):\n",
    "        if ~bases[col]:\n",
    "            sentense += \" \" + df.iloc[row, col]\n",
    "    sentenses.append(sentense)\n",
    "print(sentenses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['not_flagged' 'flagged' 'flagged' 'not_flagged' 'not_flagged']\n"
     ]
    }
   ],
   "source": [
    "labels = np.array(df.iloc[:, 1])\n",
    "print(labels[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 896
    }
   ],
   "source": [
    "labels = np.where(labels == \"not_flagged\", 0, labels)\n",
    "labels = np.where(labels == \"flagged\", 1, labels)\n",
    "labels = np.int32(labels)\n",
    "labels"
   ]
  },
  {
   "source": [
    "2. data split"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3)\n",
    "tf.random.set_seed(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "total: 80, train: 64, test: 16\ntotal: 80, train: 64, test: 16\n Having self harmed in the past,  I shared my experience with a friend in hopes of swaying her from those actions.\n1\n"
     ]
    }
   ],
   "source": [
    "train_s, test_s, train_l, test_l = train_test_split(sentenses, labels, test_size=0.2)\n",
    "print(f\"total: {len(sentenses)}, train: {len(train_s)}, test: {len(test_s)}\",\n",
    "      f\"total: {len(labels)}, train: {len(train_l)}, test: {len(test_l)}\",\n",
    "      test_s[0], test_l[0], sep=\"\\n\")"
   ]
  },
  {
   "source": [
    "3. tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 899,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer()\n",
    "token.fit_on_texts(train_s)\n",
    "# print(token.word_index)"
   ]
  },
  {
   "source": [
    "추측하자면, test sentenses에 대한 sequence 변환은 train을 통해 만들어진 word_index에 있는 것들만을 기준으로 한다. 그래서 확실하게 fit이랑 to(alike transform)를 나눠놓은 듯하다."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Over the internet a LOT of people write to me to talk about their problems. Its often girl related, I dont know, I guess it can be nice to rant to someone you dont know irl/someone who can look at the situation a little bit more objective? I think a lot of guys also feel that it can be hard to talk to girl friends irl about their innermost feelings sometimes, thats the feeling I get at least. My friend jokingly refers to me as an Agony Aunt\"  because so many people come to me with their problems. All I do is listen     \n[87, 5, 159, 4, 88, 6, 19, 250, 1, 14, 1, 37, 33, 38, 46, 160, 117, 67, 251, 2, 118, 89, 2, 161, 16, 56, 22, 252, 1, 253, 1, 57, 58, 118, 89, 162, 57, 42, 56, 119, 47, 5, 120, 4, 121, 163, 90, 254, 2, 48, 4, 88, 6, 255, 164, 68, 17, 16, 56, 22, 165, 1, 37, 1, 67, 13, 162, 33, 38, 256, 257, 91, 258, 5, 166, 2, 69, 47, 259, 8, 18, 260, 261, 1, 14, 15, 92, 262, 263, 70, 43, 71, 19, 93, 1, 14, 9, 38, 46, 72, 2, 264, 49, 122]\n"
     ]
    }
   ],
   "source": [
    "x = token.texts_to_sequences(train_s)\n",
    "print(train_s[0], x[0], sep=\"\\n\")"
   ]
  },
  {
   "source": [
    "다음 연산을 쉽게 하기 위해 변수로 저장"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = len(token.word_index) + 1"
   ]
  },
  {
   "source": [
    "max_value = len(x[0])\n",
    "for idx in np.arange(1, len(x)):\n",
    "    if (max_value < len(x[idx])):\n",
    "        max_value = len(x[idx])\n",
    "        print(idx, max_value)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 902,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19 304\n"
     ]
    }
   ]
  },
  {
   "source": [
    "4. padding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0  87   5 159   4  88   6  19 250   1  14   1  37  33  38  46 160\n 117  67 251   2 118  89   2 161  16  56  22 252   1 253   1  57  58 118\n  89 162  57  42  56 119  47   5 120   4 121 163  90 254   2  48   4  88\n   6 255 164  68  17  16  56  22 165   1  37   1  67  13 162  33  38 256\n 257  91 258   5 166   2  69  47 259   8  18 260 261   1  14  15  92 262\n 263  70  43  71  19  93   1  14   9  38  46  72   2 264  49 122]\n"
     ]
    }
   ],
   "source": [
    "x = pad_sequences(x, max_value)\n",
    "print(x[0])"
   ]
  },
  {
   "source": [
    "### word embedding을 포함한 모형 설계"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout #, Conv2D, MaxPooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.callbacks import EarlyStopping"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 904,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping = EarlyStopping(patience=5, monitor=\"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(word_count, 10, input_length=max_value))\n",
    "model.add(Conv1D(32, kernel_size=5, activation=\"relu\"))\n",
    "model.add(Conv1D(64, kernel_size=5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 51 samples, validate on 13 samples\n",
      "Epoch 1/20\n",
      "51/51 [==============================] - 1s 20ms/step - loss: 0.6843 - accuracy: 0.5882 - val_loss: 0.6304 - val_accuracy: 0.6923\n",
      "Epoch 2/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.6378 - accuracy: 0.6863 - val_loss: 0.6275 - val_accuracy: 0.6923\n",
      "Epoch 3/20\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.6227 - accuracy: 0.6863 - val_loss: 0.6146 - val_accuracy: 0.6923\n",
      "Epoch 4/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5977 - accuracy: 0.6863 - val_loss: 0.6126 - val_accuracy: 0.6923\n",
      "Epoch 5/20\n",
      "51/51 [==============================] - 0s 5ms/step - loss: 0.6036 - accuracy: 0.6863 - val_loss: 0.6104 - val_accuracy: 0.6923\n",
      "Epoch 6/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5707 - accuracy: 0.7059 - val_loss: 0.6062 - val_accuracy: 0.6923\n",
      "Epoch 7/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.6149 - accuracy: 0.6863 - val_loss: 0.6019 - val_accuracy: 0.6923\n",
      "Epoch 8/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5761 - accuracy: 0.7059 - val_loss: 0.6058 - val_accuracy: 0.6923\n",
      "Epoch 9/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5584 - accuracy: 0.7059 - val_loss: 0.5879 - val_accuracy: 0.6923\n",
      "Epoch 10/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.5304 - accuracy: 0.7255 - val_loss: 0.5622 - val_accuracy: 0.6923\n",
      "Epoch 11/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.4828 - accuracy: 0.7843 - val_loss: 0.5387 - val_accuracy: 0.6154\n",
      "Epoch 12/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.3812 - accuracy: 0.8431 - val_loss: 0.5400 - val_accuracy: 0.6923\n",
      "Epoch 13/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.3114 - accuracy: 0.8431 - val_loss: 0.4624 - val_accuracy: 0.7692\n",
      "Epoch 14/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.2202 - accuracy: 0.9804 - val_loss: 0.4713 - val_accuracy: 0.6923\n",
      "Epoch 15/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.1887 - accuracy: 0.9804 - val_loss: 0.4659 - val_accuracy: 0.6923\n",
      "Epoch 16/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.1527 - accuracy: 0.9608 - val_loss: 0.4821 - val_accuracy: 0.8462\n",
      "Epoch 17/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.0709 - accuracy: 0.9804 - val_loss: 0.3737 - val_accuracy: 0.7692\n",
      "Epoch 18/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.0722 - accuracy: 0.9804 - val_loss: 0.3565 - val_accuracy: 0.8462\n",
      "Epoch 19/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.0355 - accuracy: 1.0000 - val_loss: 0.3684 - val_accuracy: 0.8462\n",
      "Epoch 20/20\n",
      "51/51 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.3927 - val_accuracy: 0.8462\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x22215911ec8>"
      ]
     },
     "metadata": {},
     "execution_count": 907
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])#, \"mse\"])\n",
    "model.fit(x, train_l, validation_split=0.2, batch_size=10, epochs=20, callbacks=[stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "64/64 [==============================] - 0s 312us/step\n",
      "Accuracy: 0.9688\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %.4f\" %model.evaluate(x, train_l)[1])"
   ]
  },
  {
   "source": [
    "모델 평가 기록을 하자.\n",
    "\n",
    "- 0.7143: Embedding(10) -> Flatten() -> Dense(1)\n",
    "- 0.2857: Embedding(10) -> Flatten() -> Dense(max_value) -> Dense(1)\n",
    "- 0.8214: Embedding(10) -> Flatten() -> Dense(max_value) -> Dense(8) -> Dense(1)\n",
    "- 0.7500: Embedding(10) -> Conv1D(64, 4) -> MaxPooling1D(2) -> Flatten() -> Dense(max_value) -> Dense(8) -> Dense(1)\n",
    "\n",
    "validation_split 적용\n",
    "\n",
    "- 0.9464: Embedding(10) -> Conv1D(64, 4) -> MaxPooling1D(2) -> Flatten() -> Dense(max_value) -> Dropout(0.3) -> Dense(10) -> Dense(1, \"sigmoid\")\n",
    "\n",
    "EarlyStopping 적용\n",
    "\n",
    "- 0.7321: Embedding(8) -> Conv1D(64, 4) -> MaxPooling1D(2) -> Flatten() -> Dropout(0.3) -> Dense(10) -> Dense(1, \"sigmoid\")\n",
    "- 0.9286: Embedding(8) -> Conv1D(64, 4) -> Conv1D(128, 4) -> MaxPooling1D(2) -> Flatten() -> Dense(128) -> Dropout(0.3) -> Dense(10) -> Dense(1, \"sigmoid\")\n",
    "\n",
    "이젠 아주 감각적으로 하려고 노력\n",
    "\n",
    "- 0.9531: Embedding(8) -> Conv1D(32, 5) -> Conv1D(64, 5) -> MaxPooling1D(2) -> Dropout(0.3) -> Flatten() -> Dense(128) -> Dropout(0.3) -> Dense(10) -> Dense(1, \"sigmoid\")\n",
    "- 0.9688: \n",
    "model = Sequential()\n",
    "model.add(Embedding(word_count, 10, input_length=max_value))\n",
    "model.add(Conv1D(32, kernel_size=5, activation=\"relu\"))\n",
    "model.add(Conv1D(64, kernel_size=5, activation=\"relu\"))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(10, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "stopping = EarlyStopping(patience=5, monitor=\"val_loss\")\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(x, train_l, validation_split=0.2, batch_size=10, epochs=20, callbacks=[stopping])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "tt = token.texts_to_sequences(test_s)\n",
    "tt = pad_sequences(tt, max_value)\n",
    "print(\"Accuracy: %.4f\" % model.evaluate(tt, test_l)[1])"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 909,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "16/16 [==============================] - 0s 623us/step\n",
      "Accuracy: 0.8750\n"
     ]
    }
   ]
  }
 ]
}