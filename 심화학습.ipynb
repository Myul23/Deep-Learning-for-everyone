{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 오차 역전파의 계산법"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "은닉층의 노드마다 x -> sum(w * x + b) = z -> activation(z) = y -> y\r\n",
    "\r\n",
    "W_31(t + 1) = W_31(t) - (loss에 대한 W_31 편미분계수)\r\n",
    "\r\n",
    "- 오차 역전파이므로 출력층에서부터 계산\r\n",
    "- 오차, loss = 평균 오차 제곱합\r\n",
    "\r\n",
    "연쇄 법칙, chain rule\r\n",
    "- 하나의 가중치에 대한 편미분계수를 계산하려면, 이후 거치는 모든 연산에 대한 미분계수가 필요하다.<br />(합성함수의 미분계수는 각 계산의 미분계수를 곱해야 한다)\r\n",
    "\r\n",
    "<pre>\r\n",
    "bias는 graph를 좌우로 움직이는 역할을 하지.\r\n",
    "활성화 함수로 사용되는 sigmoid가 가장 안정된 예측을 하게 하는 bias 값이 1이다.\r\n",
    "무엇보다 미분에서 상수항은 사라지기 때문에 근본적으로 어떤 값이든 상관이 없다.\r\n",
    "</pre>\r\n",
    "\r\n",
    "출력층이라 다른 class의 loss를 고려할 필요가 업다면\r\n",
    "- *W_31(t + 1) = W_31(t) - (y - yt) * y * (1 - y) * x*\r\n",
    "- 대게의 경우, 편미분에서 class별 loss가 사라지지 않아 직접하지 않고, 앞에 식에 이번 편미분계수를 곱하는 것으로 한다.\r\n",
    "\r\n",
    "가중치 update를 보다 빠르게 연산하고자 node마다 매 udpate step에서 동일하게 사용되는 구간을 delta 식으로 기억해둔다.<br />(이때 delta 식은 out * (1 - out)의 형태)\r\n",
    "\r\n",
    "-> W_11(t + 1) = W_11(t) - delta_h * x"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 파이썬 코드로 확인하는 신경망"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import numpy as np\r\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### custom"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "data = [[[0, 0], [0]], [[0, 1], [1]], [[1, 0], [1]], [[1, 1], [0]]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "lr = 0.1\r\n",
    "mo = 0.4\r\n",
    "iterations = 5000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "random.seed(777)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### custom function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "def makeMatrix(i, j, fill=0.0):\r\n",
    "    mat = []\r\n",
    "    for i in range(i):\r\n",
    "        mat.append([fill] * j)\r\n",
    "    return mat"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "def sigmoid(x, derivative=False):\r\n",
    "    if derivative == True:\r\n",
    "        return x * (1 - x)\r\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def tanh(x, derivative=False):\r\n",
    "    if derivative == True:\r\n",
    "        return 1 - x ** 2\r\n",
    "    return np.tanh(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model\r\n",
    "\r\n",
    "초깃값 지정 -> 업데이트 함수 -> 역전파 함수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "class NeuralNetwork:\r\n",
    "    def __init__(self, num_x, num_yh, num_y0, bias=1):\r\n",
    "        # variables\r\n",
    "        self.num_x = num_x + bias\r\n",
    "        self.num_yh = num_yh\r\n",
    "        self.num_y0 = num_y0\r\n",
    "\r\n",
    "        # activation function\r\n",
    "        self.activation_input = [1.0] * self.num_x\r\n",
    "        self.activation_hidden = [1.0] * self.num_yh\r\n",
    "        self.activation_out = [1.0] * self.num_y0\r\n",
    "\r\n",
    "        # weights\r\n",
    "        self.weight_in = makeMatrix(self.num_x, self.num_yh)\r\n",
    "        for i in range(self.num_x):\r\n",
    "            for j in range(self.num_yh):\r\n",
    "                self.weight_in[i][j] = random.random()\r\n",
    "\r\n",
    "        self.weight_out = makeMatrix(self.num_yh, self.num_y0)\r\n",
    "        for i in range(self.num_yh):\r\n",
    "            for j in range(self.num_y0):\r\n",
    "                self.weight_out[i][j] = random.random()\r\n",
    "\r\n",
    "        # before weight init for momentum SGD\r\n",
    "        self.gradient_in = makeMatrix(self.num_x, self.num_yh)\r\n",
    "        self.gradient_out = makeMatrix(self.num_yh, self.num_y0)\r\n",
    "\r\n",
    "    # https://goo.gl/f6khsU 참조\r\n",
    "    def update(self, inputs):\r\n",
    "        # 입력층 activation function\r\n",
    "        for i in range(self.num_x - 1):\r\n",
    "            self.activation_input[i] = inputs[i]\r\n",
    "\r\n",
    "        # 은닉층 activation function\r\n",
    "        for i in range(self.num_yh):\r\n",
    "            sum = 0.0\r\n",
    "            for j in range(self.num_x):\r\n",
    "                sum += self.activation_input[j] * self.weight_in[j][i]\r\n",
    "            self.activation_hidden[i] = tanh(sum, False)\r\n",
    "\r\n",
    "        # 출력층 activation function\r\n",
    "        for i in range(self.num_y0):\r\n",
    "            sum = 0.0\r\n",
    "            for j in range(self.num_yh):\r\n",
    "                sum += self.activation_hidden[j] * self.weight_out[j][i]\r\n",
    "            self.activation_out[i] = tanh(sum, False)\r\n",
    "        return self.activation_out[:]\r\n",
    "\r\n",
    "    # momentum back-propagation\r\n",
    "    def backPropagate(self, targets):\r\n",
    "        # 출력층 delta식\r\n",
    "        output_deltas = [0.0] * self.num_y0\r\n",
    "        for k in range(self.num_y0):\r\n",
    "            error = targets[k] - self.activation_out[k]\r\n",
    "            output_deltas[k] = tanh(self.activation_out[k],  True) * error\r\n",
    "\r\n",
    "        # 은닉층 delta식\r\n",
    "        hidden_deltas = [0.0] * self.num_yh\r\n",
    "        for j in range(self.num_yh):\r\n",
    "            error = 0.0\r\n",
    "            for k in range(self.num_y0):\r\n",
    "                error += output_deltas[k] * self.weight_out[j][k]\r\n",
    "            hidden_deltas[j] = tanh(self.activation_hidden[j], True) * error\r\n",
    "\r\n",
    "        # 출력층 update\r\n",
    "        for j in range(self.num_yh):\r\n",
    "            for k in range(self.num_y0):\r\n",
    "                gradient = output_deltas[k] * self.activation_hidden[j]\r\n",
    "                v = mo * self.gradient_out[j][k] - lr * gradient\r\n",
    "                self.weight_out[j][k] += v\r\n",
    "                self.gradient_out[j][k] = gradient\r\n",
    "\r\n",
    "        # 입력층 update\r\n",
    "        for i in range(self.num_x):\r\n",
    "            for j in range(self.num_yh):\r\n",
    "                gradient = hidden_deltas[j] * self.activation_input[i]\r\n",
    "                v = mo * self.gradient_in[i][j] - lr * gradient\r\n",
    "                self.weight_in[i][j] += v\r\n",
    "                self.gradient_in[i][j] = gradient\r\n",
    "\r\n",
    "        error = 0.0\r\n",
    "        for k in range(len(targets)):\r\n",
    "            error += 0.5 * (targets[k] - self.activation_out[k]) ** 2\r\n",
    "        return error\r\n",
    "\r\n",
    "    def train(self, iterations, patterns):\r\n",
    "        for i in range(iterations):\r\n",
    "            error = 0.0\r\n",
    "            for p in patterns:\r\n",
    "                inputs = p[0]\r\n",
    "                targets = p[1]\r\n",
    "                self.update(inputs)\r\n",
    "                error += self.backPropagate(targets)\r\n",
    "\r\n",
    "            if i % 500 == 0:\r\n",
    "                print(\"error: %-.5f\" % error)\r\n",
    "\r\n",
    "    def result(self, patterns):\r\n",
    "        for p in patterns:\r\n",
    "            print(\"Input: %s, Predict: %s\" % (p[0], self.update(p[0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "if __name__ == \"__main__\":\r\n",
    "    n = NeuralNetwork(2, 2, 1)\r\n",
    "    n.train(iterations, data)\r\n",
    "    n.result(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "error: 0.66537\n",
      "error: 0.00263\n",
      "error: 0.00088\n",
      "error: 0.00051\n",
      "error: 0.00036\n",
      "error: 0.00027\n",
      "error: 0.00022\n",
      "error: 0.00018\n",
      "error: 0.00016\n",
      "error: 0.00014\n",
      "Input: [0, 0], Predict: [0.0006183430577843577]\n",
      "Input: [0, 1], Predict: [0.9889696478602484]\n",
      "Input: [1, 0], Predict: [0.9889970505963889]\n",
      "Input: [1, 1], Predict: [0.0021449252379778148]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "97ae724bfa85b9b34df7982b8bb8c7216f435b92902d749e4263f71162bea840"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}